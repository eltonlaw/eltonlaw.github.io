2:I[5613,[],""]
3:I[1778,[],""]
4:I[5250,["250","static/chunks/250-92aea3426083640b.js","124","static/chunks/app/wiki/page-9812511c8467e6eb.js"],""]
0:["TdA9xTPNvnWb6-5OopSiP",[[["",{"children":["wiki",{"children":["large_language_models",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",{"children":["wiki",{"children":["large_language_models",{"children":["__PAGE__",{},["$L1",[["$","div",null,{"className":"postFrontmatter","children":[["$","h2",null,{"className":"PostFrontmatter_postTitle__fY2bg","children":"Large Language Models"}],["$","span",null,{"className":"PostFrontmatter_postDate__ncWjv","children":"Last Updated: 2023-12-19 5:59PM"}]]}],"\n",["$","h3",null,{"children":"Inference Pipeline"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["Raw strings are mapped to tokens in a vocabulary where a vocab looks like ",["$","code",null,{"children":"{\" \": 0,  \"lo\": 1, \"hel\": 2, \"r\": 3, \"wo\": 4, \"ld\": 5}"}]," and the resulting tokens for ",["$","code",null,{"children":"\"hello world\""}]," would be ",["$","code",null,{"children":"[2, 1, 0, 4, 3, 5]"}],". Part of GPT-2's vocab is below. It uses Byte-Pair Encoding so tokens are parts of words. There are a lot of ",["$","code",null,{"children":"Ġ"}]," characters and those are special, indicating the start of a new word. ",["$","code",null,{"children":"Ġaut"}]," creates a token for the first 3 letters of\"author\" whereas ",["$","code",null,{"children":"aut"}]," wouldn't, instead ",["$","code",null,{"children":"aut"}],"'s value would substituted in the middle for a word like \"nautilus\". There's usually a padding and truncation element so that inputs are normalized to be a certain shape."]}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"children":"In [1]: from transformers import AutoTokenizer\nIn [2]: tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nIn [3]: tokenizer.get_vocab()\nOut[3]:\n{'Ġaut': 1960,\n 'roleum': 21945,\n '151': 24309,\n 'ascal': 27747,\n 'azeera': 28535,\n 'Ġchore': 30569,\n '][': 7131,\n 'ĠEns': 48221,\n ...}\nIn [4]: len(tokenizer.get_vocab())\nOut[4]: 50279\nIn [5]: tokenizer.encode(\"hello world\", return_tensors=\"pt\")\nOut[5]: tensor([[31373,   995]])\nIn [6]: tokenizer.encode(\"hello world hello world\", return_tensors=\"pt\")\nOut[6]: tensor([[31373,   995, 23748, 995]])\nIn [7]: tokenizer.encode(\"hello world\", return_tensors=\"pt\", max_length=10, padding=\"max_length\", truncation=True)\nOut[7]: tensor([[31373,   995, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]])\nIn [8]: tokenizer.encode(\"hello world\", return_tensors=\"pt\", max_length=10, padding=\"max_length\", truncation=True)\nOut[8]: tensor([[31373]])\n"}]}],"\n",["$","ol",null,{"start":"2","children":["\n",["$","li",null,{"children":["Tokens are converted into a numerical representation known as an embedding. Each token would turn into something like ",["$","code",null,{"children":"[0.14321, 0.098342, -1.12378 ...]"}],"). GPT-2 has a vocab size of 50257, and embeddings have size of 768 so the embedding layer is ",["$","code",null,{"children":"(n_vocab, n_embeddings)"}],". (",["$","code",null,{"children":"wte"}]," stands for word token embeddings)"]}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"children":"In [1]: from transformers import AutoModel\n\nIn [2]: model = AutoModel.from_pretrained(\"gpt2\")\n\nIn [3]: model.wte\nOut[3]: Embedding(50257, 768)\n"}]}],"\n",["$","ol",null,{"start":"3","children":["\n",["$","li",null,{"children":"Feedforward through some fully connected layers and self attention layers"}],"\n",["$","li",null,{"children":["Head layer is of shape ",["$","code",null,{"children":"(n_embedding, n_vocab)"}]]}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"children":"In [2]: model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\nIn [3]: model.lm_head\nOut[3]: Linear(in_features=768, out_features=50257, bias=False)\n"}]}],"\n",["$","ol",null,{"start":"5","children":["\n",["$","li",null,{"children":"Softmax of output values, use token associated with max value as the next token"}],"\n"]}],"\n",["$","h3",null,{"children":"Finetuning"}],"\n",["$","p",null,{"children":"Finetuning is when you start with an already trained model as opposed to a randomly initialized model. The benefits to this are that you're able to leverage larger general datasets, leverage models trained already and use less resources. Available approaches:"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":"Keep pretrained model frozen, remove logits layer, train a classifier on the embedding"}],"\n",["$","li",null,{"children":"Keep pretrained model frozen, attach extra hidden layers to the end"}],"\n",["$","li",null,{"children":"Only freeze parts of the pretrained model, rest is trained, lots of variety in how the selection happens"}],"\n",["$","li",null,{"children":"Don't freeze the model, use pretrained model as the initialization values. More prone to catastrophic forgetting."}],"\n"]}],"\n",["$","h3",null,{"children":"Uncategorized Notes"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["Usually the more layers you update the better the performance ",["$","sup",null,{"children":["$","a",null,{"href":"#user-content-fn-1","id":"user-content-fnref-1","data-footnote-ref":true,"aria-describedby":"footnote-label","children":"1"}]}]]}],"\n",["$","li",null,{"children":"Task-specific datasets and finetuning required. This is difficult because lots of tasks are going to be very difficult to collect datasets for."}],"\n",["$","li",null,{"children":"The more complicated the task and the less data there is, the more likely the model will fall-back on things it learned during pretraining. Humans don't require large supervised datasets to learn most language tasks, at most a tiny number of demonstrations is often enough."}],"\n",["$","li",null,{"children":"Unsupervised pre-training teaches the model a broad amount of skills and at inference time we can use some combination of those to do the task we give it."}],"\n",["$","li",null,{"children":"By giving models a task description we can have higher accuracy than without the task description up to a certain point where enough examples of the task have been given."}],"\n",["$","li",null,{"children":"Meta-learning/Zero-shot transfer is for the idea of providing task specific inputs + instruction only at inference time. There is zero-shot, one-shot, few-shot for how many demonstrations are provided at runtime. Zero and one-shot are relevant distinctions because they're closer to human learning. These *-shot learnings are not competing alternatives but have different problem settings."}],"\n",["$","li",null,{"children":"Curated datasets (using similarity to hand-picked elements) perform better than unfiltered or lightly filtered verions of the same dataset."}],"\n",["$","li",null,{"children":"T5 beam search using a beam width of 4 and a length penalty of alpha = 0.6"}],"\n",["$","li",null,{"children":"\"Larger models can typically use a larger batch size but require a smaller learning rate\""}],"\n"]}],"\n",["$","section",null,{"data-footnotes":true,"className":"footnotes","children":[["$","h2",null,{"className":"sr-only","id":"footnote-label","children":"Footnotes"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"id":"user-content-fn-1","children":["\n",["$","p",null,{"children":["\"Finetuning Open-Source LLMs.\" Youtube, uploaded by Sebastian Raschka, 14 Oct. 2023, ",["$","a",null,{"href":"https://youtu.be/gs-IDg-FoIQ?si=OCUI22mSHWSfmFK6&t=375","children":"https://youtu.be/gs-IDg-FoIQ?si=OCUI22mSHWSfmFK6&t=375"}]," ",["$","a",null,{"href":"#user-content-fnref-1","data-footnote-backref":"","aria-label":"Back to reference 1","className":"data-footnote-backref","children":"↩"}]]}],"\n"]}],"\n"]}],"\n"]}]],null]]},["$","$L2",null,{"parallelRouterKey":"children","segmentPath":["children","wiki","children","large_language_models","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/bd17e4652862e3d3.css","precedence":"next","crossOrigin":""}]]}]]},["$","$L2",null,{"parallelRouterKey":"children","segmentPath":["children","wiki","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},[null,["$","html",null,{"lang":"en","children":["$","body",null,{"children":["$","div",null,{"className":"RootLayout_container__tH3RP","children":[["$","div",null,{"className":"RootLayout_masthead__LsRd7","children":[["$","h3",null,{"className":"$undefined","children":[["$","$L4",null,{"className":"RootLayout_titleLink__VQPJ9","href":"/","children":"eltonlaw"}],["$","small",null,{"children":" sundries"}]]}],["$","$L4",null,{"className":"RootLayout_titleLink__VQPJ9","href":"https://github.com/eltonlaw","children":["$","svg",null,{"viewBox":"0 0 16 16","width":"16px","height":"16px","children":["$","path",null,{"fill":"#828282","d":"M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"}]}]}]]}],["$","$L2",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]]}]}]}],null]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5d82fb85d7740057.css","precedence":"next","crossOrigin":""}]],"$L5"]]]]
5:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"eltonlaw"}]]
1:null
